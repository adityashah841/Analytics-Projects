---
title: "15 Year stock data"
author: "Aditya Shah"
date: "2025-05-04"
output: word_document
---

```{r}
library(tidyverse)
library(lubridate)
library(xts)
library(zoo)
library(scales)
library(ggfortify)
library(tidyquant)
library(PerformanceAnalytics)
library(TTR)
library(forecast)
library(tseries)
library(timetk)
library(prophet)
library(Metrics)
```

### Step 1: Load the data and convert to a longer format instead of the wide format

```{r}
df_raw <- read_csv("15 Years Stock Data of NVDA AAPL MSFT GOOGL and AMZN.csv",
                   col_types = cols(Date = col_date(format = "%Y-%m-%d")))
```
```{r}
df_long <- df_raw %>%
  pivot_longer(-Date,
               names_to = c(".value", "Ticker"),
               names_sep = "_") %>%
  arrange(Ticker, Date)

df_long %>% filter(Ticker=="AAPL") %>% slice_head(n=5)
```
### Step 2: EDA: Time-series plots of closing prices (linear and log-scaled)
```{r}
ggplot(df_long, aes(x = Date, y = Close, color = Ticker)) +
  geom_line(linewidth = 0.3) +
  labs(
    title = "Big - Tech Closing Prices (2010–2025)",
    x     = "Date",
    y     = "Closing Price (USD)"
  ) +
  theme_minimal()
```

```{r}
ggplot(df_long, aes(Date, Close, color = Ticker)) +
  geom_line() +
  scale_y_log10(labels = dollar_format()) +
  labs(title="Log-scale Closing Prices (2010–2025)",
       y="Close (USD)", x="")
```
### Summary statistics by ticker (min, max, mean, sd, etc.)
```{r}
df_long %>%
  group_by(Ticker) %>%
  summarize(
    min = min(Close), max = max(Close),
    mean = mean(Close), sd = sd(Close)
  )
```
### Returns and Volatility: Computing daily log-returns and plotting rolling volatility (30-day rolling std. dev.)
```{r}
df_returns <- df_long %>%
  group_by(Ticker) %>%
  arrange(Date) %>%
  mutate(log_return = log(Close) - log(lag(Close))) %>%
  drop_na()

df_vol <- df_returns %>%
  group_by(Ticker) %>%
  mutate(vol30 = rollapply(log_return, 30, sd, align="right", fill=NA))
  
ggplot(df_vol, aes(Date, vol30, color=Ticker)) +
  geom_line() +
  labs(title="30-Day Rolling Volatility",
       y="Std. Dev. of Log-Returns", x="")
```
### Clear volatility clusters around major market events—e.g. the 2020 COVID crash, late-2018 sell-off, etc.
### NVDA (purple) generally has higher peaks (it’s the most volatile), while AAPL and MSFT tend to sit lower.
### Volatility tends to revert: after a big spike it slowly falls back toward a baseline.

```{r}
ggplot(df_returns, aes(x = Date, y = log_return, color = Ticker)) +
  geom_line(linewidth = 0.1) +
  labs(
    title = "Daily Log-Returns for Big-Tech Stocks",
    x     = "Date",
    y     = "Log-Return"
  ) +
  theme_minimal()
```
### The series is mean-reverting around zero - stocks don’t trend upward day-to-day in raw returns.
### NVDA’s spikes are visibly larger and more frequent, again highlighting its higher risk.

```{r}
ggplot(df_returns, aes(x = log_return)) +
  geom_histogram(bins = 50, alpha = 0.7) +
  facet_wrap(~Ticker, scales = "free") +
  labs(
    title = "Distribution of Daily Log-Returns",
    x     = "Log-Return",
    y     = "Count"
  ) +
  theme_minimal()
```
### All five distributions are centered near zero (as expected).
### NVDA has the widest spread (fatter tails), confirming it has more extreme days.
### AAPL/MSFT are the tightest—lower day-to-day variability.


### Assessment
### Relative Risk: NVDA > GOOGL ≈ AMZN > MSFT ≈ AAPL
### Event Impact: Market shocks (e.g., COVID) cause synchronized spikes in volatility across all tickers.

```{r}
df_norm <- df_long %>%
  group_by(Ticker) %>%
  mutate(idx_close = Close / first(Close) * 100)

ggplot(df_norm, aes(Date, idx_close, color=Ticker)) +
  geom_line() +
  labs(title="Indexed Close (100 = Jan 1 2010)",
       y="Index Value", x="")
```
### NVIDIA (NVDA) has absolutely dominated: rising from 100 to over 30 000 by 2025 (≈ 300× growth).
### The next best is Apple (AAPL) at roughly 350–400, then Amazon (AMZN) around 250–300, Microsoft (MSFT) ~200–250, and Google (GOOGL) ~150–200.

### Timing of the outperformance
### Prior to 2018, all five were roughly in lock-step. NVDA’s real “take-off” begins around 2018, and especially post-2020 (AI/machine-learning boom). AAPL/AMZN/MSFT also accelerate in the late-2010s, but much more moderately.
```{r}
ret_wide <- df_returns %>%
  select(Date, Ticker, log_return) %>%
  pivot_wider(values_from=log_return, names_from=Ticker)

corr_mat <- cor(ret_wide %>% select(-Date), use="pairwise.complete.obs")

# Heatmap
corr_mat %>%
  as_tibble(rownames="Ticker1") %>%
  pivot_longer(-Ticker1, names_to="Ticker2", values_to="r") %>%
  ggplot(aes(Ticker1, Ticker2, fill=r)) +
    geom_tile() + geom_text(aes(label=round(r,2))) +
    scale_fill_gradient2(midpoint=0, low="blue", high="red", mid="white") +
    labs(title="Return Correlation (2010–2025)")
```
### Most pairwise correlations sit in the 0.5–0.6 range, meaning they tend to rise and fall together, but not perfectly. The highest linkage is GOOGL–MSFT (≈ 0.64) and AAPL–MSFT (≈ 0.59)—those two have been especially in sync.

### Since no two are perfectly correlated (no 1.0 off the diagonal), there is some benefit to holding a basket of these stocks. NVDA is the least correlated on average (its lowest pairwise of ≈ 0.47 with AMZN), so adding NVDA may give us the biggest incremental diversification.

### A 0.5–0.6 correlation means market-wide shocks (e.g. 2020 COVID crash) hit them all, but each also has its own idiosyncratic drivers (earnings surprises, product launches, etc.) that cause them to diverge at times.
```{r}
pca <- prcomp(ret_wide %>% select(-Date), scale=TRUE, center=TRUE)
summary(pca)  # variance explained

# Biplot of PC1 vs. PC2
autoplot(pca, data=ret_wide, loadings=TRUE, loadings.label=TRUE)

```
### PC1 explains about 64% of all the day‐to‐day return variation across the five stocks.
### In the biplot, all five arrows are roughly collinear along PC1 (all pointing leftward), and of similar length. This means they all load strongly and comparably on PC1 - it’s capturing the “common” or market-wide moves that drive them in tandem. In practical terms, a single factor already gets us two-thirds of the variability.


### PC2 adds another 11%, bringing cumulative explained variance to ~75%. NVDA’s arrow points upward, while AMZN (and to a lesser extent GOOGL) point downward. This orthogonal axis is picking up the contrast between high-flyers like NVIDIA vs. the more muted movers like Amazon. When PC2 is positive, NVDA tends to be outperforming the group; when PC2 is negative, AMZN (and its peers) are relatively stronger.

### We can conclusively say that 2 components explain ~75% of the total variance. We can project your 5-D return data into a 2-D space (PC1 vs. PC2) and still retain most of the information. These 2 dimensions can be described as follows:

### PC1 is effectively a “common Big-Tech factor”
### PC2 is a “differential performance factor” (splitting NVDA vs. the others)
### PC3 also has an approximately equal contribution to the variance as compared to PC2 and hence it should also be considered for a better explainability.


### PC4 & PC5 (together ~15%) capture more idiosyncratic or noise‐level variation.

```{r}
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
round(var_explained, 4)
cat("PC3 explains", scales::percent(var_explained[3]), "of total variance.\n")
```
```{r}
pc3_loadings <- pca$rotation[, 3]
round(pc3_loadings, 3)
```
```{r}
autoplot(pca, x = 1, y = 3, loadings = TRUE, loadings.label = TRUE) +
  labs(
    title = "PCA Biplot: PC1 (63.8%) vs PC3 (10.2%)",
    x = paste0("PC1 (", scales::percent(var_explained[1]), ")"),
    y = paste0("PC3 (", scales::percent(var_explained[3]), ")")
  ) +
  theme_minimal()
```
### PC3 explains about 10.2% of the day-to-day return variation
### Bringing PC1+PC2+PC3 together gets us about 85% of all the variability in the five stock series.
### Apple (AAPL) has the largest positive loading (≈+0.76). NVIDIA (NVDA) has a sizable negative loading (≈–0.55), with Amazon also mildly negative (≈–0.33). Google and Microsoft are effectively zero on this axis.

### Days with high PC3 scores are ones where AAPL outperforms the group (especially relative to NVDA). Days with low PC3 scores are ones where NVDA (and to some extent AMZN) outperform AAPL.



###Event Study (Covid crash)

Estimation window (≈14 months) gives us enough “normal” data to fit each stock’s market-model (α, β).
Event window (approx. 6 weeks) spans the crash (Feb 20) through the initial recovery (Mar 31).

```{r}
est_start <- as.Date("2019-01-01")
est_end   <- as.Date("2020-02-19")
evt_start <- as.Date("2020-02-20")
evt_end   <- as.Date("2020-03-31")
```

Using SPY (S&P 500), since it is a broad liquid proxy for the overall US markets and calculating the log-returns since they are usually symmetric for gains and losses 
```{r}
spy <- tq_get("SPY",
              from = est_start - months(1),  # get a bit of cushion
              to   = evt_end) %>%
  select(date, close) %>%
  mutate(mkt_ret = log(close) - log(lag(close))) %>%
  drop_na() %>%
  rename(Date = date, SPY_ret = mkt_ret)
```
Joining the SPY returns data with the stock data by date
```{r}
full_ret <- df_returns %>%
  inner_join(spy, by = "Date")
```

Estimate α (intercept) & β (slope) per stock over the estimation window

  α captures each stock’s average return when the market is flat.
  β measures each stock’s sensitivity to market moves.

```{r}
params <- full_ret %>%
  filter(Date >= est_start, Date <= est_end) %>%
  group_by(Ticker) %>%
  summarize(
    alpha = coef(lm(log_return ~ SPY_ret, data = cur_data()))[1],
    beta  = coef(lm(log_return ~ SPY_ret, data = cur_data()))[2]
  )
```

Computing abnormal returns of the stocks and the Cumulative Abnormal Returns (CAR) of these stocks over time

```{r}
ar <- full_ret %>%
  filter(Date >= evt_start, Date <= evt_end) %>%
  left_join(params, by = "Ticker") %>%
  mutate(
    expected_ret = alpha + beta * SPY_ret,
    abnormal_ret = log_return - expected_ret
  ) %>%
  group_by(Ticker) %>%
  arrange(Date) %>%
  mutate(CAR = cumsum(abnormal_ret))
```
```{r}
ggplot(ar, aes(x = Date, y = CAR, color = Ticker)) +
  geom_line(size = 1) +
  labs(
    title = "COVID-19 Crash Event Study: Cumulative Abnormal Returns",
    x     = "Date",
    y     = "Cumulative Abnormal Return"
  ) +
  theme_minimal()
```
### The event window starts on Feb 20th, which is approximately when the pandemic was declared. CAR is approximately 0 for all the tickers because they start with 0 (That's the way it was created). In the initial parts of the graph we see that all the stocks under-perform on SPY as the markets fell and these tech names fell even more than expected given their β values.

### This was then followed by Rapid Rebound & Divergence. NVDA (purple) shoots up fastest: huge abnormal gains as it rallies more strongly than the market (likely fueled by expectations of accelerated demand in GPUs/AI). AAPL (red) and AMZN (olive) also turn positive—outperforming SPY, but less dramatically. GOOGL (green) and MSFT (teal) lag—they only modestly outperform or briefly underperform SPY.

### Mid-March Peaks: Market and stocks are both volatile, but NVDA and AMZN carve out the largest CAR peaks (~ +0.50 and +0.35 respectively). AAPL peaks around +0.27, MSFT +0.20, GOOGL +0.12.

### Late-March Consolidation: All CARs pull back slightly but remain positive, meaning every Big-Tech name ultimately beat SPY over this crash/recovery window. Looking at their final standings, we can say that the Nvidia stock gave about +40% cumulative abnormal returns (CAR), Amazon stock gave about +20%, Apple & Micfrosoft gave about +15% and Google gave +5% over the SPY returns.


### Even as the overall market plunged, these tech giants recovered faster than predicted by their normal market sensitivity—most notably NVDA.

### Relative winners and losers:
### NVIDIA was the clear superstar, generating ~40% more return than a pure SPY‐β prediction. Amazon and Apple also showed strong resilience (>15% extra). Google barely outperformed SPY, suggesting a more muted recovery.







### Modeling daily returns
```{r}
rets_list <- df_returns %>%
  select(Date, Ticker, log_return) %>%
  group_split(Ticker) %>%
  setNames(unique(df_returns$Ticker))
```
```{r}
arima_results <- map(rets_list, function(df) {
  df_ts <- tk_ts(df, select = log_return, start = c(year(min(df$Date)), 
                                                   yday(min(df$Date))), 
                frequency = 252)
  fit    <- auto.arima(df_ts)
  fc     <- forecast(fit, h = 20)   # 20 trading days ahead
  forecast::accuracy(fc)                      # in-sample accuracy metrics
  list(model = fit, forecast = fc)
})
```
```{r}
aapl_fc <- arima_results$AAPL$forecast

autoplot(aapl_fc) +
  labs(
    title = "AAPL Log-Return ARIMA Forecast (20-day)",
    x     = "Trading Day",
    y     = "Log-Return"
  ) +
  theme_minimal()
```

```{r}
googl_fc <- arima_results$GOOGL$forecast

autoplot(googl_fc) +
  labs(
    title = "GOOGL Log-Return ARIMA Forecast (20-day)",
    x     = "Trading Day",
    y     = "Log-Return"
  ) +
  theme_minimal()
```
```{r}
nvda_fc <- arima_results$NVDA$forecast

autoplot(nvda_fc) +
  labs(
    title = "NVDA Log-Return ARIMA Forecast (20-day)",
    x     = "Trading Day",
    y     = "Log-Return"
  ) +
  theme_minimal()
```
```{r}
msft_fc <- arima_results$MSFT$forecast

autoplot(msft_fc) +
  labs(
    title = "MSFT Log-Return ARIMA Forecast (20-day)",
    x     = "Trading Day",
    y     = "Log-Return"
  ) +
  theme_minimal()
```
```{r}
amzn_fc <- arima_results$AMZN$forecast

autoplot(amzn_fc) +
  labs(
    title = "AMZN Log-Return ARIMA Forecast (20-day)",
    x     = "Trading Day",
    y     = "Log-Return"
  ) +
  theme_minimal()
```

### Observing these plots, we can say that the daily log-returns is a seriees that is more or less cenetered around zero and does not carry any time series patterns and proves the Efficient Market Hypothesis correct. In addition to this, we can also assess that the NVDA stock has the maximum volatility amongst the selected top tech stocks.

### These ARIMA models try to model the mean of the various series and prove that there doesn't exist any time series pattern in it. We can also try modelling the variance of the daily log-returns series using the GARCH models. GARCH modelling will be used later for risk analysis.

### We can also switch to Price-Level models for predictive analysis.



FB PROPHET modelling

```{r}
tickers <- c('AAPL', 'GOOGL', 'NVDA', 'MSFT', 'AMZN')
prophet_forecasts <- lapply(tickers, function(tk) {
  df <- df_long %>% filter(Ticker==tk) %>% select(ds=Date, y=Close)
  m  <- prophet(df, daily.seasonality=TRUE)
  fut <- make_future_dataframe(m, periods=60)
  predict(m, fut)
})
names(prophet_forecasts) <- tickers
# Plot AAPL
plot(prophet(df_long %>% filter(Ticker=="AAPL") %>% select(ds=Date, y=Close), daily.seasonality=TRUE),
     prophet_forecasts$AAPL) + ggtitle("AAPL Prophet Forecast")

```

```{r}
prophet_plot_components(prophet(df_long %>% filter(Ticker=="AAPL") %>% select(ds=Date, y=Close), daily.seasonality=FALSE), prophet_forecasts$AAPL)
```
```{r}
# Plot GOOGL
plot(prophet(df_long %>% filter(Ticker=="GOOGL") %>% select(ds=Date, y=Close), daily.seasonality=TRUE),
     prophet_forecasts$GOOGL) + ggtitle("GOOGL Prophet Forecast")

```
```{r}
prophet_plot_components(prophet(df_long %>% filter(Ticker=="GOOGL") %>% select(ds=Date, y=Close), daily.seasonality=FALSE), prophet_forecasts$GOOGL)
```
```{r}
# Plot NVDA
plot(prophet(df_long %>% filter(Ticker=="NVDA") %>% select(ds=Date, y=Close), daily.seasonality=TRUE),
     prophet_forecasts$NVDA) + ggtitle("NVDA Prophet Forecast")

```
```{r}
prophet_plot_components(prophet(df_long %>% filter(Ticker=="NVDA") %>% select(ds=Date, y=Close), daily.seasonality=FALSE), prophet_forecasts$NVDA)
```
```{r}
# Plot MSFT
plot(prophet(df_long %>% filter(Ticker=="MSFT") %>% select(ds=Date, y=Close), daily.seasonality=TRUE),
     prophet_forecasts$MSFT) + ggtitle("MSFT Prophet Forecast")

```
```{r}
prophet_plot_components(prophet(df_long %>% filter(Ticker=="MSFT") %>% select(ds=Date, y=Close), daily.seasonality=FALSE), prophet_forecasts$MSFT)
```
```{r}
# Plot AMZN
plot(prophet(df_long %>% filter(Ticker=="AMZN") %>% select(ds=Date, y=Close), daily.seasonality=TRUE),
     prophet_forecasts$AMZN) + ggtitle("AMZN Prophet Forecast")

```
```{r}
prophet_plot_components(prophet(df_long %>% filter(Ticker=="AMZN") %>% select(ds=Date, y=Close), daily.seasonality=FALSE), prophet_forecasts$AMZN)
```








ETS Modelling



```{r}
tickers <- unique(df_long$Ticker)

ets_forecasts <- lapply(tickers, function(tk) {
  prices <- df_long %>% filter(Ticker==tk) %>% pull(Close)
  ts_obj  <- ts(prices, start=c(2010,1), frequency=252)
  fit     <- ets(ts_obj)
  forecast(fit, h=60)
})

names(ets_forecasts) <- tickers

# And to plot AAPL:
autoplot(ets_forecasts$AAPL) + labs(title="AAPL ETS Forecast")
```
```{r}
autoplot(ets_forecasts$GOOGL) + labs(title="GOOGL ETS Forecast")
```
```{r}
autoplot(ets_forecasts$NVDA) + labs(title="NVDA ETS Forecast")
```
```{r}
autoplot(ets_forecasts$AMZN) + labs(title="AMZN ETS Forecast")
```
```{r}
autoplot(ets_forecasts$MSFT) + labs(title="MSFT ETS Forecast")
```
```{r}
df_hold  = tail(df_long %>% filter(Ticker=="AAPL") %>% pull(Close), 60)
prophet_hat = tail(prophet_forecasts$AAPL$yhat, 60)
ets_hat     = as.numeric(tail(ets_forecasts$AAPL$mean, 60))
list(
  Prophet_MAPE = mape(df_hold, prophet_hat),
  ETS_MAPE     = mape(df_hold, ets_hat),
  Prophet_RMSE = rmse(df_hold, prophet_hat),
  ETS_RMSE     = rmse(df_hold, ets_hat)
)
```
```{r}
df_hold  = tail(df_long %>% filter(Ticker=="GOOGL") %>% pull(Close), 60)
prophet_hat = tail(prophet_forecasts$GOOGL$yhat, 60)
ets_hat     = as.numeric(tail(ets_forecasts$GOOGL$mean, 60))
list(
  Prophet_MAPE = mape(df_hold, prophet_hat),
  ETS_MAPE     = mape(df_hold, ets_hat),
  Prophet_RMSE = rmse(df_hold, prophet_hat),
  ETS_RMSE     = rmse(df_hold, ets_hat)
)
```
```{r}
df_hold  = tail(df_long %>% filter(Ticker=="AMZN") %>% pull(Close), 60)
prophet_hat = tail(prophet_forecasts$AMZN$yhat, 60)
ets_hat     = as.numeric(tail(ets_forecasts$AMZN$mean, 60))
list(
  Prophet_MAPE = mape(df_hold, prophet_hat),
  ETS_MAPE     = mape(df_hold, ets_hat),
  Prophet_RMSE = rmse(df_hold, prophet_hat),
  ETS_RMSE     = rmse(df_hold, ets_hat)
)
```
```{r}
df_hold  = tail(df_long %>% filter(Ticker=="MSFT") %>% pull(Close), 60)
prophet_hat = tail(prophet_forecasts$MSFT$yhat, 60)
ets_hat     = as.numeric(tail(ets_forecasts$MSFT$mean, 60))
list(
  Prophet_MAPE = mape(df_hold, prophet_hat),
  ETS_MAPE     = mape(df_hold, ets_hat),
  Prophet_RMSE = rmse(df_hold, prophet_hat),
  ETS_RMSE     = rmse(df_hold, ets_hat)
)
```
```{r}
df_hold  = tail(df_long %>% filter(Ticker=="NVDA") %>% pull(Close), 60)
prophet_hat = tail(prophet_forecasts$NVDA$yhat, 60)
ets_hat     = as.numeric(tail(ets_forecasts$NVDA$mean, 60))
list(
  Prophet_MAPE = mape(df_hold, prophet_hat),
  ETS_MAPE     = mape(df_hold, ets_hat),
  Prophet_RMSE = rmse(df_hold, prophet_hat),
  ETS_RMSE     = rmse(df_hold, ets_hat)
)
```

RMSE : Root Mean Squared Error
MAPE: Mean Absolute Percentage Error


In 4 out of 5 cases (apple, amazon, microsoft and nvidia), the ETS model performs better than the FB Prophet model in identifying the trends, seasonality and the errors.

ETS excels at smoothly extrapolating recent trend and handling simple error‐trend‐seasonal structure, but it can lag sudden regime shifts. Prophet nimbly picks up abrupt changepoints and holiday effects, but may overreact to the latest jump and under‐ or over‐estimate seasonality.

Hence, an ensemble blends those complementary strengths, reducing model‐specific bias and variance.




Weighted ensemble (Inverse MAPE) for GOOGL

```{r}
library(timetk)
googl_dates <- df_long %>%
  filter(Ticker == "GOOGL") %>%
  arrange(Date) %>%
  tail(60) %>%
  pull(Date)

prop_df_GOOGL <- prophet_forecasts$GOOGL %>%
  select(ds, prophet = yhat) %>%
  mutate(ds = as.Date(ds))

ets_df_GOOGL  <- tibble(
  ds  = googl_dates,
  ets = as.numeric(ets_forecasts$GOOGL$mean)
)
```
```{r}
prophet_mape_GOOGL <- 0.08160417  
ets_mape_GOOGL <- 0.09196171

w_prophet <- (1/prophet_mape_GOOGL) / ((1/prophet_mape_GOOGL) + (1/ets_mape_GOOGL))
w_ets     <- (1/ets_mape_GOOGL)     / ((1/prophet_mape_GOOGL) + (1/ets_mape_GOOGL))
```
```{r}
ensemble_googl <- inner_join(prop_df_GOOGL, ets_df_GOOGL, by = "ds") %>%
  mutate(
    weight_prophet = w_prophet,
    weight_ets     = w_ets,
    ensemble       = weight_prophet * prophet + weight_ets * ets
  )
```
```{r}
ggplot(ensemble_googl, aes(x = ds)) +
  geom_line(aes(y = prophet, color = "Prophet")) +
  geom_line(aes(y = ets,     color = "ETS")) +
  geom_line(aes(y = ensemble, color = "Ensemble"), size = 1) +
  labs(
    title = "GOOGL 60-Day Forecast: Prophet vs ETS vs Inverse-MAPE Ensemble",
    x     = "Date", 
    y     = "Price (USD)",
    color = "Model"
  ) +
  theme_minimal()
```
```{r}
actual_googl <- df_long %>%
  filter(Ticker == "GOOGL") %>%
  arrange(Date) %>%
  tail(60) %>%
  pull(Close)

# 2) Pull out your ensemble point forecasts (must already be in the same order)
pred_googl <- ensemble_googl %>% 
  arrange(ds) %>%       # ensure sorted by date
  pull(ensemble)

# 3) Compute MAPE and RMSE
mape_googl <- mape(actual = actual_googl, predicted = pred_googl)
rmse_googl <- rmse(actual = actual_googl, predicted = pred_googl)

# 4) Print them
cat("GOOGL Ensemble MAPE:", round(mape_googl, 4), "\n")
cat("GOOGL Ensemble RMSE:", round(rmse_googl,  4), "\n")
```
### The ensemble technique for the comparable models for the GOOGL ticker has led to an approximate staggering 50% reduction in the RMSE and MAPE errors.

### For other tickers it is not useful to go for an ensemble technique since the ETS model clearly outperforms the Prophet models and an ensemble will only increase the MAPE and RMSE metrics.




Risk Assessment, Risk measure creation and portfolio simulation
```{r}
library(rugarch)
library(MASS)
```
```{r}
# 1a) GARCH spec: zero‐mean, sGARCH(1,1), Student-t
spec <- ugarchspec(
  mean.model     = list(armaOrder = c(0,0)),
  variance.model = list(model     = "sGARCH", garchOrder = c(1,1)),
  distribution.model = "std"
)

# 1b) Fit & 1-day ahead volatility forecast for each ticker
garch_fits <- map(df_returns %>% split(.$Ticker), ~ {
  ret <- .x$log_return
  fit <- ugarchfit(spec, ret, solver = "hybrid")
  f1  <- ugarchforecast(fit, n.ahead = 1)
  sigma1 <- sigma(f1)       # tomorrow's conditional sigma
  list(fit = fit, sigma1 = sigma1)
})
```
```{r}
risk_measures <- map_dfr(names(garch_fits), function(tk) {
  fit   <- garch_fits[[tk]]$fit
  sigma <- as.numeric(garch_fits[[tk]]$sigma1)
  nu    <- coef(fit)["shape"]        # Student-t ν parameter
  α     <- 0.05                      # 95% VaR
  
  q_t <- qt(α, df = nu)              # lower-tail quantile
  VaR  <- - sigma * sqrt((nu-2)/nu) * q_t
  
  ES   <- -sigma * sqrt((nu-2)/nu) * (dt(q_t, nu) / (α * (1 - 2/nu)))
  
  tibble(
    Ticker = tk,
    sigma1 = sigma,
    VaR_95 = VaR,
    ES_95  = ES
  )
})

print(risk_measures)
```

```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(quadprog)
```


```{r}
ret_cov <- df_returns %>%
  dplyr::select(Date, Ticker, log_return) %>%
  tidyr::pivot_wider(names_from = Ticker, values_from = log_return) %>%
  dplyr::select(-Date) %>%    # <— dplyr::select
  cov(use = "pairwise.complete.obs")

tickers <- colnames(ret_cov)
n       <- length(tickers)
```


Defining the different weight schemes for different portfolio strategies.

```{r}
# 1 Control: All weights are equal
w_control <- rep(1/n, n)
names(w_control) <- tickers


# 2 Inverse‐risk: use 1-day‐ahead GARCH σ₁ (from risk_measures$sigma1)
sigma1_vec <- risk_measures$sigma1
names(sigma1_vec) <- risk_measures$Ticker

w_inv_risk <- (1 / sigma1_vec)
w_inv_risk <- w_inv_risk / sum(w_inv_risk)


# 3 Return‐based: weight ∝ cumulative return over full sample
past_ret <- df_long %>%
  group_by(Ticker) %>%
  summarize(cum_ret = last(Close) / first(Close) - 1) %>%
  arrange(desc(cum_ret))

w_return <- past_ret$cum_ret / sum(past_ret$cum_ret)
names(w_return) <- past_ret$Ticker


# 4 Markowitz (mean‐variance): Optimized over maximizing returns while minimizing risk
mu_vec <- df_returns %>%
  group_by(Ticker) %>%
  summarize(mu = mean(log_return, na.rm=TRUE)) %>%
  pull(mu)
names(mu_vec) <- df_returns %>% distinct(Ticker) %>% pull(Ticker)

ret_cov <- df_returns %>%
  dplyr::select(Date, Ticker, log_return) %>%
  tidyr::pivot_wider(names_from = Ticker, values_from = log_return) %>%
  dplyr::select(-Date) %>%
  cov(use = "pairwise.complete.obs")

Sigma <- as.matrix(ret_cov)
n     <- length(mu_vec)

# 2) Build QP matrices
gamma <- 1               # risk aversion
Dmat  <- gamma * Sigma
dvec  <- mu_vec

# 3) Box constraints
max_w <- 0.30            # cap each asset at 30%

# 3a) Equality: sum(w) = 1
A_eq <- rep(1, n)
b_eq <- 1

# 3b) Lower‐bounds: w >= 0
A_lb <- diag(n)
b_lb <- rep(0, n)

# 3c) Upper‐bounds: w <= max_w  <=>  -w >= -max_w
A_ub <- -diag(n)
b_ub <- rep(-max_w, n)

# 4) Combine into Amat/bvec
#    (solve.QP expects Amat with each constraint as a COLUMN)
Amat <- cbind(
  A_eq,    # equality first
  A_lb,    # then lower‐bounds
  A_ub     # then upper‐bounds
)

bvec <- c(
  b_eq,    # sum(w)=1
  b_lb,    # w >=0
  b_ub     # -w >= -max_w
)

meq <- 1   # number of equality constraints

# 5) Solve
sol   <- solve.QP(Dmat, dvec, Amat, bvec = bvec, meq = meq)
w_mv  <- sol$solution
names(w_mv) <- names(mu_vec)

#Collecting all 4
all_weights <- list(
  Control     = w_control,
  InverseRisk = w_inv_risk,
  ReturnBased = w_return,
  MV_Optim    = w_mv
)
```


Monte-Carlo Simulations for the above created weight vector

```{r}
simulate_portfolio_returns <- function(w, Sigma, N = 5000, H = 21) {
  # w: named weight vector
  # Sigma: covariance matrix of daily log‐returns
  # N: number of simulated paths
  # H: horizon in trading days
  n <- length(w)
  # simulate N×H draws of multivariate normals
  sims <- mvrnorm(N, mu = rep(0, n), Sigma = Sigma)
  # repeat H times (independent days)
  rets <- replicate(H, sims, simplify = "array")  # dims N×n×H
  # For each path: sum over H days per asset → cumulative log‐return per asset
  # Actually simpler: simulate H i.i.d draws per path:
  cum_log_ret <- matrix(0, N, length(w))
  for (h in 1:H) {
    # daily returns for day h across N sims
    daily <- sims  # since identical each day (i.i.d), we can reuse sims
    # add to cumulative
    cum_log_ret <- cum_log_ret + daily
  }
  # now combine by weights: portfolio log‐return per path
  port_log_ret <- cum_log_ret %*% w
  # back to simple return:
  as.numeric(exp(port_log_ret) - 1)
}
```


Running and assessing all these simulations for varying holding periods
```{r}
horizons <- c(
  "1m"  = 21,
  "6m"  = 21 * 6,
  "1y"  = 252,
  "5y"  = 252 * 5,
  "10y" = 252 * 10
)

results <- tidyr::crossing(
  Strategy = names(all_weights),
  Horizon  = names(horizons),
  H        = horizons
) %>%
rowwise() %>%
mutate(
  sims       = list(simulate_portfolio_returns(
      w     = all_weights[[Strategy]],
      Sigma = as.matrix(ret_cov),
      N     = 5000,
      H     = H
  )),
  MeanReturn = mean(sims),
  SDReturn   = sd(sims),
  Pct5       = quantile(sims, 0.05),
  Pct95      = quantile(sims, 0.95)
) %>%
dplyr::select(Strategy, Horizon, MeanReturn, SDReturn, Pct5, Pct95)

print(results)
```

```{r}
top2_by_horizon <- results %>%
  group_by(Horizon) %>%
  slice_max(order_by = MeanReturn, n = 2) %>%
  ungroup()

print(top2_by_horizon)
```
```{r}
mv_weights_df <- tibble(
  Horizon = rep(names(horizons), each = length(w_mv)),
  Ticker  = rep(names(w_mv), times = length(horizons)),
  Weight  = rep(w_mv, times = length(horizons))
)

print(mv_weights_df)
```




















